{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f9990f-882e-4892-acfa-5541f2359e85",
   "metadata": {},
   "source": [
    "# 搭建 Triton 推理服务\n",
    "\n",
    "本节介绍如何下载、安装、配置、启动 Triton 服务，以及如何用客户端获取 Triton 的推理结果。\n",
    "\n",
    "GitHub: [triton-inference-server/server](https://github.com/triton-inference-server/server)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb2797-163b-41d5-85a7-66a7e8171920",
   "metadata": {},
   "source": [
    "## 1. 安装 Nvidia 预构建的 Docker 镜像\n",
    "\n",
    "Docker Tags: [tritonserver/tags](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags)\n",
    "\n",
    "从上述链接中，选择适合自己本地环境的 Triton 镜像。我选的是一个比较新的版本。\n",
    "\n",
    "```\n",
    "docker pull nvcr.io/nvidia/tritonserver:24.10-py3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3075c0c-2406-44d8-9afa-29324fd3f358",
   "metadata": {},
   "source": [
    "## 2. 准备模型存储库\n",
    "\n",
    "### 1）模型存储库结构\n",
    "\n",
    "模型存储库结构如下：\n",
    "\n",
    "```\n",
    "model_repository/\n",
    "  └── mlp_model/\n",
    "      ├── 1/\n",
    "      │   └── model.onnx\n",
    "      └── config.pbtxt\n",
    "```\n",
    "\n",
    "说明：\n",
    "\n",
    "- `1/`：这是模型的版本号目录，Triton 允许同一个模型存放不同的版本，这里用 1 表示第一个版本。每个版本目录下放置相应的模型文件\n",
    "- `model.onnx`：每个目录下的 ONNX 模型文件都命名为 model.onnx。你可以将你的模型重命名为 model.onnx\n",
    "\n",
    "### 2）创建配置文件\n",
    "\n",
    "每个模型需要一个配置文件 `config.pbtxt`，用于描述模型的输入、输出和其他配置信息。\n",
    "\n",
    "**MLP 模型的配置**：mlp_model/config.pbtxt\n",
    "\n",
    "```\n",
    "name: \"mlp_model\"\n",
    "backend: \"onnxruntime\"\n",
    "max_batch_size: 64\n",
    "input [\n",
    "  {\n",
    "    name: \"x\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 7 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"predict\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]\n",
    "dynamic_batching {\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n",
    "\n",
    "> **Note:** 动态批处理\n",
    ">\n",
    "> Triton Inference Server 支持动态批处理，即在短时间内将多个请求合并成一个批次进行处理，以最大化 GPU 的利用率。即使将 `max_batch_size` 设置得较大，Triton 也会根据请求的到达情况动态地合并请求，这对于提高吞吐量非常有用。\n",
    "> \n",
    "> 如需使用动态批处理，可在 `config.pbtxt` 中添加如下配置：\n",
    ">\n",
    "> ```bash\n",
    "> dynamic_batching {\n",
    "  preferred_batch_size: [ 8, 16, 32 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "> ```\n",
    "\n",
    "PS: 进入容器后，使用 `cat /opt/tritonserver/TRITON_VERSION` 命令，可查看镜像的 Triton 版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac26ba-4cda-41cd-911d-0a5c2ac8e54f",
   "metadata": {},
   "source": [
    "## 3. 启动 Triton 服务\n",
    "\n",
    "使用以下命令启动 Triton 服务：\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "    --rm \\\n",
    "    --gpus '\"device=0\"' \\\n",
    "    --shm-size=1g \\\n",
    "    -p 8000:8000 \\\n",
    "    -p 8001:8001 \\\n",
    "    -p 8002:8002 \\\n",
    "    -v /path/to/model_repository:/models \\\n",
    "    -v /path/to/workspace:/workspace \\\n",
    "    -v /path/to/data:/workspace/data \\\n",
    "    nvcr.io/nvidia/tritonserver:24.10-py3 \\\n",
    "    tritonserver \\\n",
    "        --model-repository=/models \\\n",
    "        --model-control-mode=poll \\\n",
    "        --repository-poll-secs=30\n",
    "```\n",
    "\n",
    "说明：\n",
    "\n",
    "- `--gpus '\"device=0\"'`：使用 0 号 GPU\n",
    "- `--rm`：在容器停止运行后自动删除容器\n",
    "- `-p 8000:8000、-p 8001:8001、-p 8002:8002`：映射 Triton 的 HTTP、gRPC 和 Prometheus 端口\n",
    "- `-v /path/to/model_repository:/models`：将本地模型仓库挂载到容器的 /models 目录\n",
    "- `tritonserver --model-repository=/models`：启动 Triton Server 并指定模型存储库的位置\n",
    "- `--model-control-mode=poll`：设置模型控制模式为“轮询”。这意味着当模型存储库更新时，Triton 会自动加载\n",
    "- `--repository-poll-secs=30`：设置 Triton 服务器每 30 秒轮询一次模型仓库，检查是否有模型更新\n",
    "\n",
    "注意：`/path/to/model_repository` 需要换成你的本地真实目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44c244-8315-4052-b9c3-d7a4b0ebd183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T21:59:55.728563Z",
     "iopub.status.busy": "2024-11-25T21:59:55.728563Z",
     "iopub.status.idle": "2024-11-25T21:59:55.734906Z",
     "shell.execute_reply": "2024-11-25T21:59:55.733897Z",
     "shell.execute_reply.started": "2024-11-25T21:59:55.728563Z"
    }
   },
   "source": [
    "## 4. 使用 Triton 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0035c9cd-b5b1-4c12-8f2e-7887f4c62e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.230218Z",
     "iopub.status.busy": "2024-11-29T15:13:33.230218Z",
     "iopub.status.idle": "2024-11-29T15:13:33.233067Z",
     "shell.execute_reply": "2024-11-29T15:13:33.233067Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.230218Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eecfbc6-22f8-4f30-8b36-ee50c8856e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.234073Z",
     "iopub.status.busy": "2024-11-29T15:13:33.234073Z",
     "iopub.status.idle": "2024-11-29T15:13:33.570523Z",
     "shell.execute_reply": "2024-11-29T15:13:33.570015Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.234073Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "# import json\n",
    "import tritonclient.http as httpclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed5e121-1cc0-44bd-a287-869fe614fc2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.571526Z",
     "iopub.status.busy": "2024-11-29T15:13:33.570523Z",
     "iopub.status.idle": "2024-11-29T15:13:33.573631Z",
     "shell.execute_reply": "2024-11-29T15:13:33.573631Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.571526Z"
    }
   },
   "outputs": [],
   "source": [
    "TRITON_URL = \"http://localhost:8000\"\n",
    "\n",
    "MLP_MODEL_NAME = 'mlp_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bee6ab-c853-4c00-9da0-83e2f64d94f3",
   "metadata": {},
   "source": [
    "### 1) 检查 Triton 的状态\n",
    "\n",
    "Windows 用 WSL 安装 Ubuntu：\n",
    "\n",
    "```bash\n",
    "# 查看可用的发行版\n",
    "wsl --list --online\n",
    "\n",
    "# 安装 Ubuntu\n",
    "wsl --install -d Ubuntu\n",
    "\n",
    "# 切换到 Ubuntu 并安装 curl\n",
    "sudo apt update\n",
    "sudo apt install curl -y\n",
    "\n",
    "# 下一次进入 Ubuntu 环境\n",
    "wsl -d Ubuntu\n",
    "```\n",
    "\n",
    "现在可以用 curl 测试 Triton 服务:\n",
    "\n",
    "```\n",
    "curl -v http://localhost:8000/v2\n",
    "```\n",
    "\n",
    "或者用 `requests` 库:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5d414b-1849-44df-afef-05e804318d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.574639Z",
     "iopub.status.busy": "2024-11-29T15:13:33.574639Z",
     "iopub.status.idle": "2024-11-29T15:13:33.587180Z",
     "shell.execute_reply": "2024-11-29T15:13:33.586662Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.574639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton server is ready.\n"
     ]
    }
   ],
   "source": [
    "def check_triton_health(triton_url=TRITON_URL):\n",
    "    url = f\"{triton_url}/v2/health/ready\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Triton server is ready.\")\n",
    "    else:\n",
    "        print(f\"Triton server is not ready. Status code: {response.status_code}\")\n",
    "\n",
    "check_triton_health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa57bce-0a4b-4838-a0e0-e69a8e0a469c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.587687Z",
     "iopub.status.busy": "2024-11-29T15:13:33.587687Z",
     "iopub.status.idle": "2024-11-29T15:13:33.595150Z",
     "shell.execute_reply": "2024-11-29T15:13:33.595150Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.587687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'mlp_model' is ready.\n"
     ]
    }
   ],
   "source": [
    "def check_model_health(model_name, triton_url=TRITON_URL):\n",
    "    url = f\"{triton_url}/v2/models/{model_name}/ready\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Model '{model_name}' is ready.\")\n",
    "    else:\n",
    "        print(f\"Model '{model_name}' is not ready. Status code: {response.status_code}\")\n",
    "\n",
    "check_model_health(model_name=MLP_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca66e1ef-d7eb-46da-a152-334506dea758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.596162Z",
     "iopub.status.busy": "2024-11-29T15:13:33.596162Z",
     "iopub.status.idle": "2024-11-29T15:13:33.604915Z",
     "shell.execute_reply": "2024-11-29T15:13:33.604915Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.596162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'triton',\n",
       " 'version': '2.51.0',\n",
       " 'extensions': ['classification',\n",
       "  'sequence',\n",
       "  'model_repository',\n",
       "  'model_repository(unload_dependents)',\n",
       "  'schedule_policy',\n",
       "  'model_configuration',\n",
       "  'system_shared_memory',\n",
       "  'cuda_shared_memory',\n",
       "  'binary_tensor_data',\n",
       "  'parameters',\n",
       "  'statistics',\n",
       "  'trace',\n",
       "  'logging']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_triton_meta_data(triton_url=TRITON_URL):\n",
    "    url = f\"{triton_url}/v2\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        metadata = response.json()\n",
    "        return metadata\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "get_triton_meta_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33726d-b6f5-4902-a198-f19a93d28d7d",
   "metadata": {},
   "source": [
    "## 2）HTTP/REST 客户端\n",
    "\n",
    "可以用 REST API 构造请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f7ee29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:24:36.878009Z",
     "iopub.status.busy": "2024-11-29T15:24:36.878009Z",
     "iopub.status.idle": "2024-11-29T15:24:36.887113Z",
     "shell.execute_reply": "2024-11-29T15:24:36.886025Z",
     "shell.execute_reply.started": "2024-11-29T15:24:36.878009Z"
    }
   },
   "outputs": [],
   "source": [
    "def client(input_data,\n",
    "           input_name,\n",
    "           model_name,\n",
    "           datatype='FP32',\n",
    "           triton_url=TRITON_URL,\n",
    "           model_version='1'):\n",
    "    url = f\"{triton_url}/v2/models/{model_name}/versions/{model_version}/infer\"\n",
    "\n",
    "    # 构造请求体\n",
    "    inputs = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": input_name,\n",
    "                \"shape\": list(input_data.shape),\n",
    "                \"datatype\": datatype,\n",
    "                \"data\": input_data.flatten().tolist()\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 发送 POST 请求\n",
    "    response = requests.post(url=url, json=inputs)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Inference request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bca55314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:24:37.791649Z",
     "iopub.status.busy": "2024-11-29T15:24:37.790512Z",
     "iopub.status.idle": "2024-11-29T15:24:37.803116Z",
     "shell.execute_reply": "2024-11-29T15:24:37.802432Z",
     "shell.execute_reply.started": "2024-11-29T15:24:37.791649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'mlp_model',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'predict',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 2],\n",
       "   'data': [0.21256819367408752, -0.06383021175861359]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造输入\n",
    "input_data = np.array([np.random.rand(7).astype(np.float32)])\n",
    "\n",
    "# 发送推理请求\n",
    "output = client(input_data=input_data,\n",
    "                input_name=\"x\",\n",
    "                model_name=MLP_MODEL_NAME)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7acc06-15d8-4845-ad11-82f9a2a76a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:06:28.000605Z",
     "iopub.status.busy": "2024-11-29T15:06:27.999614Z",
     "iopub.status.idle": "2024-11-29T15:06:28.002810Z",
     "shell.execute_reply": "2024-11-29T15:06:28.002810Z",
     "shell.execute_reply.started": "2024-11-29T15:06:28.000605Z"
    }
   },
   "source": [
    "### 3） httpclient 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed56d45-37b8-49b8-979d-ca9de25e9e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.628550Z",
     "iopub.status.busy": "2024-11-29T15:13:33.627541Z",
     "iopub.status.idle": "2024-11-29T15:13:33.639177Z",
     "shell.execute_reply": "2024-11-29T15:13:33.639177Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.628550Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建客户端\n",
    "client = httpclient.InferenceServerClient(\"localhost:8000\")\n",
    "\n",
    "# help(client.infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63781214-66ac-4700-a69f-d0b94402b4e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.641184Z",
     "iopub.status.busy": "2024-11-29T15:13:33.640181Z",
     "iopub.status.idle": "2024-11-29T15:13:33.658714Z",
     "shell.execute_reply": "2024-11-29T15:13:33.658714Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.641184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造输入\n",
    "input_data = np.array([np.random.rand(7).astype(np.float32)])\n",
    "\n",
    "# 请求推理\n",
    "inputs = [httpclient.InferInput(\"x\", input_data.shape, \"FP32\")]\n",
    "inputs[0].set_data_from_numpy(input_data)\n",
    "\n",
    "outputs = [httpclient.InferRequestedOutput(\"predict\")]\n",
    "\n",
    "response = client.infer(MLP_MODEL_NAME,\n",
    "                        inputs=inputs,\n",
    "                        model_version='1',\n",
    "                        outputs=outputs)\n",
    "text_features = response.as_numpy(\"predict\")\n",
    "\n",
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00427fce-5288-44ec-a88a-8fdfe1c04688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T15:13:33.659725Z",
     "iopub.status.busy": "2024-11-29T15:13:33.659725Z",
     "iopub.status.idle": "2024-11-29T15:13:33.664223Z",
     "shell.execute_reply": "2024-11-29T15:13:33.663719Z",
     "shell.execute_reply.started": "2024-11-29T15:13:33.659725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25953004, -0.10462983]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41adf5-42a4-43dd-b50a-cf7e4f5cf0f9",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "- [triton-inference-server/tutorials](https://github.com/triton-inference-server/tutorials)\n",
    "- [model_configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa38dfb-2dc1-48c7-ac8e-d3cfc79ce7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
