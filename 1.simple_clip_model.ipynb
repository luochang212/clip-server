{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9931e805-e562-434a-87e5-f7de1e7617ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T15:11:08.477034Z",
     "iopub.status.busy": "2024-10-31T15:11:08.476039Z",
     "iopub.status.idle": "2024-10-31T15:11:08.483557Z",
     "shell.execute_reply": "2024-10-31T15:11:08.482550Z",
     "shell.execute_reply.started": "2024-10-31T15:11:08.477034Z"
    }
   },
   "source": [
    "# 简单的 CLIP 模型\n",
    "\n",
    "[CLIP](https://openai.com/index/clip/) 是一个多模态模型。它能将图像和文本映射到同一个向量空间中，由此可以产生诸多应用。比如，通过计算图片与文本的相似性，可以用近似最近邻 (ANN) 从相册中检索与给定 query 语义相近的图片。此外，CLIP 的 Vision Encoder 可以作为特征提取器使用，用于生成的图像 Embedding。如果在 Vision Encoder 后加一个 fc 层，并且冻住骨干网络仅对 fc 层做训练，通常可以得到一个效果不错的图像分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc21df0-c92a-471a-b3c8-920fdb4284f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:11:45.831104Z",
     "iopub.status.busy": "2024-11-29T14:11:45.830105Z",
     "iopub.status.idle": "2024-11-29T14:11:45.834218Z",
     "shell.execute_reply": "2024-11-29T14:11:45.834218Z",
     "shell.execute_reply.started": "2024-11-29T14:11:45.831104Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip setuptools -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !python -m pip install git+https://github.com/openai/CLIP.git -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install accelerate\n",
    "# !pip install -U flash-attn --no-build-isolation\n",
    "# https://visualstudio.microsoft.com/visual-cpp-build-tools/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dba0b63-2017-4a93-8c56-1cd96e5fffa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:11:45.835224Z",
     "iopub.status.busy": "2024-11-29T14:11:45.835224Z",
     "iopub.status.idle": "2024-11-29T14:11:50.045876Z",
     "shell.execute_reply": "2024-11-29T14:11:50.045876Z",
     "shell.execute_reply.started": "2024-11-29T14:11:45.835224Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "import utils\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd90ceed-4e22-4bd8-a101-018f03a8cf20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:11:50.046881Z",
     "iopub.status.busy": "2024-11-29T14:11:50.046881Z",
     "iopub.status.idle": "2024-11-29T14:11:50.051015Z",
     "shell.execute_reply": "2024-11-29T14:11:50.051015Z",
     "shell.execute_reply.started": "2024-11-29T14:11:50.046881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'workspace'\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "model_path = utils.gen_abspath(directory='./', rel_path=MODEL_PATH)\n",
    "img_path = utils.gen_abspath(directory=DATA_PATH, rel_path='cat.JPG')\n",
    "\n",
    "# 设置日志记录\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0951663-2028-4d5d-ba6e-45c732363f1f",
   "metadata": {},
   "source": [
    "## 1. 使用 transformers 加载 CLIP\n",
    "\n",
    "### 1）CLIP 模型介绍\n",
    "\n",
    "为了训练 CLIP，OpenAI 收集了 4 亿对图文数据进行训练。训练目标是让图片的特征向量与对应文本的特征向量在向量空间中靠得更近。训练采用多模态对比学习的方法。在一个 batch 中，对于每张图片，它的目标是找到当前 batch 中与之最匹配的文本，最大化与匹配文本的相似度（正样本），并同时最小化与其他文本的相似度（负样本）。\n",
    "\n",
    "CLIP 训练了两个独立的编码器：\n",
    "\n",
    "- **图像编码器**：通常使用 ResNet 或 Vision Transformer (ViT)。\n",
    "- **文本编码器**：基于 Transformer 结构。\n",
    "\n",
    "OpenAI 尝试了多种编码器，得出一个很直觉的结论：模型的效果与参数量呈现正相关。基本上使用参数越大的编码器，效果就越好。\n",
    "\n",
    "### 2）用 CLIP 计算图文相似性分数\n",
    "\n",
    "用 transformers 库加载 openai/clip-vit-base-patch32。并用一张猫的图片与两句话进行对比：\n",
    "\n",
    "- a photo of a cat\n",
    "- a photo of a dog\n",
    "\n",
    "使用 CLIP 模型，计算猫的图片与每句话的相似性分数，取分数最高的句子作为图片的分类标签。验证模型能否有效区分猫和狗。\n",
    "\n",
    "> **Note:** 值得注意的是 a photo of {item} 是一种 Prompt Engineer 方法。除了前面这个，OpenAI 还用了很多其他标签。比如：\n",
    "> \n",
    "> ```\n",
    "> a bad photo of a {}.\n",
    "> a photo of many {}.\n",
    "> a sculpture of a {}.\n",
    "> a photo of the hard to see {}.\n",
    "> a low resolution photo of the {}.\n",
    "> a rendering of a {}.\n",
    "> graffiti of a {}.\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43dcc70e-cbe5-46c7-a9ee-eb41276a3d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:11:50.052027Z",
     "iopub.status.busy": "2024-11-29T14:11:50.052027Z",
     "iopub.status.idle": "2024-11-29T14:11:57.681387Z",
     "shell.execute_reply": "2024-11-29T14:11:57.680384Z",
     "shell.execute_reply.started": "2024-11-29T14:11:50.052027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luoch\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=model_path,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", cache_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0142d51-3dbd-413c-9c98-c5b2f0b9dbd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:11:57.682389Z",
     "iopub.status.busy": "2024-11-29T14:11:57.681387Z",
     "iopub.status.idle": "2024-11-29T14:11:57.935382Z",
     "shell.execute_reply": "2024-11-29T14:11:57.934878Z",
     "shell.execute_reply.started": "2024-11-29T14:11:57.682389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "                   images=Image.open(img_path),\n",
    "                   return_tensors=\"pt\",\n",
    "                   padding=True).to(device)\n",
    "\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd545250-b574-4153-8bbe-654e8c257435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:11:57.937387Z",
     "iopub.status.busy": "2024-11-29T14:11:57.937387Z",
     "iopub.status.idle": "2024-11-29T14:11:58.443734Z",
     "shell.execute_reply": "2024-11-29T14:11:58.443734Z",
     "shell.execute_reply.started": "2024-11-29T14:11:57.937387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9941, 0.0056]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.autocast(device):\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a1b73-6ece-4db4-8bb3-92134dfb7a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T19:19:28.774508Z",
     "iopub.status.busy": "2024-11-25T19:19:28.773508Z",
     "iopub.status.idle": "2024-11-25T19:19:28.784080Z",
     "shell.execute_reply": "2024-11-25T19:19:28.782069Z",
     "shell.execute_reply.started": "2024-11-25T19:19:28.773508Z"
    }
   },
   "source": [
    "## 2. 使用 CLIP 库加载 ViT 视觉编码模型\n",
    "\n",
    "使用 CLIP 库加载一个使用 Vision Transformer (ViT) 作为视觉编码器的 CLIP 模型。\n",
    "\n",
    "将图、文分别转换为 `image_features` 和 `text_features` 两个 Embedding，再计算相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1366815-826f-4d77-a290-a12beb209872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:11:58.444737Z",
     "iopub.status.busy": "2024-11-29T14:11:58.444737Z",
     "iopub.status.idle": "2024-11-29T14:12:00.659813Z",
     "shell.execute_reply": "2024-11-29T14:12:00.659813Z",
     "shell.execute_reply.started": "2024-11-29T14:11:58.444737Z"
    }
   },
   "outputs": [],
   "source": [
    "# 下载模型\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, download_root=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f15209-e82d-44db-8f73-9e016e64171f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:12:00.660817Z",
     "iopub.status.busy": "2024-11-29T14:12:00.660817Z",
     "iopub.status.idle": "2024-11-29T14:12:00.665818Z",
     "shell.execute_reply": "2024-11-29T14:12:00.665818Z",
     "shell.execute_reply.started": "2024-11-29T14:12:00.660817Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_image_embedding(image_path):\n",
    "\n",
    "    # 加载示例图像并处理\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"The image file {image_path} was not found.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while opening the image: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(image.shape)\n",
    "\n",
    "    # 使用模型生成图像的 Embedding\n",
    "    with torch.amp.autocast(device_type=device):  # 使用混合精度推理\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "    \n",
    "    # 将 Embedding 转换为标准化的向量\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features\n",
    "\n",
    "def generate_text_embedding(text_list):\n",
    "\n",
    "    # 将文本转化为 tokens\n",
    "    text_tokens = clip.tokenize(text_list).to(device)\n",
    "\n",
    "    print(text_tokens.shape)\n",
    "\n",
    "    # 使用模型生成文本的 Embedding\n",
    "    with torch.amp.autocast(device_type=device):  # 使用混合精度推理\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    # 将 Embedding 转换为标准化的向量\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4dd69f-0990-40bf-8dfc-c8c8e2a98a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:12:00.667044Z",
     "iopub.status.busy": "2024-11-29T14:12:00.665818Z",
     "iopub.status.idle": "2024-11-29T14:12:00.684026Z",
     "shell.execute_reply": "2024-11-29T14:12:00.684026Z",
     "shell.execute_reply.started": "2024-11-29T14:12:00.667044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 77])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 512]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取文本 Embedding\n",
    "# text_features = generate_text_embedding([\"A photo of a cat!\", \"A photo of a dog\"])\n",
    "text_features = generate_text_embedding([\"一只猫\", \"一条狗\"])\n",
    "text_features.shape, text_features.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10bfa626-d2c6-41dc-9cfa-210792da3726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:12:00.685030Z",
     "iopub.status.busy": "2024-11-29T14:12:00.685030Z",
     "iopub.status.idle": "2024-11-29T14:12:00.820330Z",
     "shell.execute_reply": "2024-11-29T14:12:00.820330Z",
     "shell.execute_reply.started": "2024-11-29T14:12:00.685030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取图片 Embedding\n",
    "\n",
    "image_features = generate_image_embedding(img_path)\n",
    "image_features.shape, image_features.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed545ad-37c2-4c4b-b74a-739fa5045832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T14:12:00.821673Z",
     "iopub.status.busy": "2024-11-29T14:12:00.821673Z",
     "iopub.status.idle": "2024-11-29T14:12:00.827047Z",
     "shell.execute_reply": "2024-11-29T14:12:00.827047Z",
     "shell.execute_reply.started": "2024-11-29T14:12:00.821673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.281 , 0.2598]], dtype=float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算图像与文本的相似性分数\n",
    "eventual_similarity = torch.matmul(image_features, text_features.T)\n",
    "eventual_similarity.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe961fb-c9e6-436f-a530-b7d25a7445d4",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "- [OpenAI CLIP 官网](https://openai.com/index/clip/)\n",
    "- [Hugging Face CLIP](https://huggingface.co/docs/transformers/en/model_doc/clip)\n",
    "- [GitHub CLIP](https://github.com/openai/CLIP)\n",
    "- [arXiv: *Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/abs/2103.00020)\n",
    "\n",
    "视频资源：\n",
    "\n",
    "- [CLIP 论文逐段精读](https://www.bilibili.com/video/BV1SL4y1s7LQ)\n",
    "- [CLIP 改进工作串讲（上）](https://www.bilibili.com/video/BV1FV4y1p7Lm)\n",
    "- [CLIP 改进工作串讲（下）](https://www.bilibili.com/video/BV1gg411U7n4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab63147-ecff-43ed-9aa5-5f38a21b6c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
